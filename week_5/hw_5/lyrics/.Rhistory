rownames(TopWords) = colnames(doc.tfidf)[2:(n+1)]
TopWords = droplevels(TopWords)
kable(TopWords)
#TF-IDF Hours 文章取得的重要關鍵字 TDM merge 視覺化
TDM$d = as.character(TDM$d)
AllTop = as.data.frame( table(as.matrix(TopWords)) )
AllTop = AllTop[order(AllTop$Freq, decreasing = TRUE),]
kable(head(AllTop))
TopNo = 5
tempGraph = data.frame()
for( t in c(1:TopNo) )
{
word = matrix( rep(c(as.matrix(AllTop$Var1[t])), each = n), nrow = n )
temp = cbind( colnames(doc.tfidf)[2:(n+1)], t(TDM[which(TDM$d == AllTop$Var1[t]), 2:(n+1)]), word )
colnames(temp) = c("hour", "freq", "words")
tempGraph = rbind(tempGraph, temp)
names(tempGraph) = c("hour", "freq", "words")
}
library(ggplot2)
library(varhandle)
tempGraph$freq = unfactor(tempGraph$freq)
ggplot(tempGraph, aes(hour, freq)) +
geom_point(aes(color = words, shape = words), size = 5) +
geom_line(aes(group = words, linetype = words))
kable(tail(AllTop))
#發文時間與發文量
filenames = as.array(paste0("./DATA/",colnames(doc.tfidf)[2:(n+1)],".txt"))
sizeResult = apply(filenames, 1, file.size) / 1024
showSize = data.frame(colnames(doc.tfidf)[2:(n+1)], sizeResult)
names(showSize) = c("hour", "size_KB")
ggplot(showSize, aes(x = hour, y = size_KB)) + geom_bar(stat="identity")
url.list[1]['XMLAbstractNode']
url.list[1]
str(url.list[1])
as.character(url.list[1])
as.String(url.list[1])
unlist(url.list[1])
xpath= '//*[contains(concat( " ", @class, " " ), concat( " ", "hotli_R", " " ))]','//a'
xpath= '//*[contains(concat( " ", @class, " " ), concat( " ", "hotli_R", " " ))]','//a'
url.list <- xpathSApply( html, xpath )
View(url.list)
xpath= c('//*[contains(concat( " ", @class, " " ), concat( " ", "hotli_R", " " ))]','//a')
url.list <- xpathSApply( html, xpath )
url.list
xpath= c('//*[contains(concat( " ", @class, " " ), concat( " ", "hotli_R", " " ))]//a')
url.list <- xpathSApply( html, xpath )
url.list
#PTT 網路爬蟲抓出所有歌詞頁面連結所對應的網址
url  <- 'https://mojim.com/twh100012.htm'
html <- htmlParse( GET(url) )
xpath= '//*[contains(concat( " ", @class, " " ), concat( " ", "hc4", " " ))]//a | //*[contains(concat( " ", @class, " " ), concat( " ", "hc3", " " ))]//a'
url.list <- xpathSApply( html, xpath, xmlAttrs )
url.list
#發現有雜訊，取出網址
urls= list()
haha= function(a){paste0('https:/mojim.com',a[['href']])}
urls= lapply(url.list, haha)
urls
#PTT 網路爬蟲抓出所有歌詞頁面連結所對應的網址
url  <- 'https://mojim.com/twzhot-song.htm'
html <- htmlParse( GET(url) )
xpath= '//*[(@id = "mx5_A") and (((count(preceding-sibling::*) + 1) = 1) and parent::*)]//a'
url.list <- xpathSApply( html, xpath, xmlAttrs )
url.list
#發現有雜訊，取出網址
urls= list()
haha= function(a){paste0('https:/mojim.com',a[['href']])}
urls= lapply(url.list, haha)
urls
xpath= '//*[(@id = "mx5_A") and (((count(preceding-sibling::*) + 1) = 1) and parent::*)]//a[(((count(preceding-sibling::*) + 1) = 1) and parent::*)]'
url.list <- xpathSApply( html, xpath, xmlAttrs )
url.list
#發現有雜訊，取出網址
urls= list()
haha= function(a){paste0('https:/mojim.com',a[['href']])}
urls= lapply(url.list, haha)
url.list
str(url.list)
View(url.list)
url.list[1]
url.list[1,]
url.list= url.list[1,]
#發現有雜訊，取出網址
urls= list()
haha= function(a){paste0('https:/mojim.com',a)}
urls= lapply(url.list, haha)
urls
urls
urls= unlist(urls)
urls
#
library(dplyr)
xpath2= '//*[(@id = "fsZx3")]'
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, xpth2, xmlValue )
name <- paste0(substr(url, 21,32), ".txt")
write(doc, name)
}
lapply(urls,getdoc)
reader= function(u){
text= read_html(u) %>% html_nodes("#fsZx3") %>% html_text()
write(text,paste0(u,'.txt'))}
lapply(urls,reader)
reader= function(u){
text= read_html(u) %>% html_nodes("#fsZx3") %>% html_text()
write(text,'.txt')}
lapply(urls,reader)
lapply(urls,reader)
lapply(urls,reader)
reader<- function(u){
text<- read_html(u) %>% html_nodes("#fsZx3") %>% html_text()
#name=
write(text,'.txt')}
lapply(urls,reader)
reader<- function(u){
text<- read_html(u) %>% html_nodes("#fsZx3") %>% html_text()
#name=
write(text,'.txt')}
reader<- function(u){
text<- read_html(u) %>% html_nodes("#fsZx3") %>% html_text()
#name=
write(text,'.txt')}
lapply(urls,reader)
reader<- function(u){
text<- read_html(u) %>% html_nodes("#fsZx3") %>% html_text()
name= read_html(u) %>% html_nodes("#fsZx2") %>% html_text()
Name= paste0(name,'txt')
write(text,Name)}
lapply(urls,reader)
reader<- function(u){
text<- read_html(u) %>% html_nodes("#fsZx3") %>% html_text()
name= read_html(u) %>% html_nodes("#fsZx2") %>% html_text()
Name= paste0(name,'txt')
write(text,Name)}
lapply(urls,reader)
#PTT Boy-Girl 分析
#載入所需的套件包
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
reader<- function(u){
text<- read_html(u) %>% html_nodes("#fsZx3") %>% html_text()
name= read_html(u) %>% html_nodes("#fsZx2") %>% html_text()
Name= paste0(name,'txt')
write(text,Name)}
lapply(urls,reader)
#reader<- function(u){
text<- read_html(https:/mojim.com/twy106207x3x2.htm) %>% html_nodes("#fsZx3") %>% html_text()
#reader<- function(u){
text<- read_html('https:/mojim.com/twy106207x3x2.htm') %>% html_nodes("#fsZx3") %>% html_text()
#reader<- function(u){
doc<- read_html('https:/mojim.com/twy106207x3x2.htm') %>% html_nodes("#fsZx3") %>% html_text()
library(rvest)
library(magrittr)
#reader<- function(u){
doc<- read_html('https:/mojim.com/twy106207x3x2.htm') %>% html_nodes("#fsZx3") %>% html_text()
library(rvest)
library(magrittr)
#reader<- function(u){
doc<- read_html('https:/mojim.com/twy106207x3x2.html') %>% html_nodes("#fsZx3") %>% html_text()
read_html('https:/mojim.com/twy106207x3x2.htm') %>% html_nodes("#fsZx3") %>% html_text()
title=read_html("https://technews.tw/", options = 'NOBLANKS') %>%
html_nodes(".entry-title a , #album_test1 a , #album_test1 h3") %>%
html_text() %>% iconv("UTF-8")
#reader<- function(u){
doc<- read_html('https://mojim.com/twy106207x3x2.htm') %>% html_nodes("#fsZx3") %>% html_text()
haha= function(a){paste0('https://mojim.com',a)}
urls= lapply(url.list, haha)
urls
urls= unlist(urls)
read_html('https:/mojim.com/twy106207x3x2.htm') %>% html_nodes("#fsZx3") %>% html_text()
read_html('https://mojim.com/twy106207x3x2.htm') %>% html_nodes("#fsZx3") %>% html_text()
read_html('https://mojim.com/twy106207x3x2.htm') %>% html_nodes("#fsZx2") %>% html_text()
reader<- function(u){
doc<- read_html(u) %>% html_nodes("#fsZx3") %>% html_text()
name= read_html(u) %>% html_nodes("#fsZx2") %>% html_text()
Name= paste0(name,'txt')
write(text,Name)}
lapply(urls,reader)
reader<- function(u){
doc<- read_html(u) %>% html_nodes("#fsZx3") %>% html_text()
name= read_html(u) %>% html_nodes("#fsZx2") %>% html_text()
Name= paste0(name,'.txt')
write(text,Name)}
lapply(urls,reader)
reader= function(u){
doc= read_html(u) %>% html_nodes("#fsZx3") %>% html_text()
title= read_html(u) %>% html_nodes("#fsZx2") %>% html_text()
name= paste0(title,'.txt')
write(text,Name)}
lapply(urls,reader)
reader= function(u){
doc= read_html(u) %>% html_nodes("#fsZx3") %>% html_text()
title= read_html(u) %>% html_nodes("#fsZx2") %>% html_text()
name= paste0(title,'.txt')
write(text,name)}
lapply(urls,reader)
urls
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, xpth2, xmlValue )
title= xpathSApply(html, xpath3, xmlValue)
name <- paste0(title, ".txt")
write(doc, name)
}
lapply(urls,getdoc)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, xpath2, xmlValue )
title= xpathSApply(html, xpath3, xmlValue)
name <- paste0(title, ".txt")
write(doc, name)
}
lapply(urls,getdoc)
reader= function(u){
doc= read_html(u) %>% html_nodes("#fsZx3") %>% html_text()
title= read_html(u) %>% html_nodes("#fsZx2") %>% html_text()
name= paste0(title,'.txt')
write(text,name)}
lapply(urls,reader)
doc
title
title= read_html(u) %>% html_nodes("#fsZx2") %>% html_text()
urls
title= read_html('https://mojim.com/twy109369x14x1.htm') %>% html_nodes("#fsZx2") %>% html_text()
title
name= paste0(title,'.txt')
name
write(text,name)}
write(text,name)#}
name= paste0(u,'.txt')
name= paste0('https://mojim.com/twy109369x14x1.htm','.txt')
name
write(text,name)#}
name= paste0('./DATA/', title, ".txt")
write(text,name)#}
name= paste0(title, ".txt")
write(text,name)#}
write(text,'1.txt')#}
doc
write(text,'1.txt')#}
#利用所有文章的網址去抓所有文章內文, 並解析出文章的內容並依照 hour 合併儲存。
library(dplyr)
write(text,'1.txt')#}
write(text,'1.txt',append = TRUE)#}
help(write)
write.table(text,'1.txt',append = TRUE)#}
str(doc)
write.table(text,'1.txt',append = TRUE)#}
write.table(text,'1.txt',append = F)#}
write.table(text,'1.txt',append = F)#}
write.table(c(text),'1.txt')#}
doc
xpath2= '//*[(@id = "fsZx3")]'
xpath3= '//*[(@id = "fsZx2")]'
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, xpath2, xmlValue )
title= xpathSApply(html, xpath3, xmlValue)
name <- paste0(title, ".txt")
write(doc, name)
}
lapply(urls,getdoc)
xpath2= '//*[(@id = "fsZx3")]'
xpath3= '//*[(@id = "fsZx2")]'
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, xpath2, xmlValue )
title= xpathSApply(html, xpath3, xmlValue)
name <- paste0(title, ".txt")
write(doc, name)
}
urls
lapply(urls,getdoc)
setwd("D:/CSX_Lyhs/week_5/hw_5/lyrics")
#利用所有文章的網址去抓所有文章內文, 並解析出文章的內容並依照 hour 合併儲存。
path = "./"
dir = DirSource(path), encoding = "UTF-8")
dir = DirSource(path, encoding = "UTF-8")
dir
dir = DirSource("./", encoding = "UTF-8")
dir
d.corpus <- Corpus( DirSource("./") )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
d.corpus
d.corpus[1]
#進行斷詞，並依照日期建立文本矩陣 TermDocumentMatrix
mixseg = worker()
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
n = length(seg)
TDM = tokens[[1]]
colNames <- names(seg)
colNames <- gsub(".txt", "", colNames)
for( id in c(2:n) )
{
TDM = merge(TDM, tokens[[id]], by="d", all = TRUE)
names(TDM) = c('d', colNames[1:id])
}
TDM[is.na(TDM)] <- 0
library(knitr)
kable(head(TDM))
kable(tail(TDM))
#建立文本資料結構與基本文字清洗
dir = DirSource(paste(path,"pos/",sep=""), encoding = "UTF-8")
#建立文本資料結構與基本文字清洗
dir = DirSource("./"), encoding = "UTF-8")
#建立文本資料結構與基本文字清洗
dir = DirSource("./", encoding = "UTF-8")
dir
corpus = Corpus(dir)
length(corpus)
corpus[[1]]
worker(corpus)
cc= worker()
cc[corpus]
corpus[[1]]
str(corpus[[1]])
cc[unlist(corpus)]
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, function(word) {
corpus <- tm_map(corpus, function(word) {
corpus <- tm_map(corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)})
cc[unlist(corpus)]
cc= worker()
cc[unlist(corpus)]
>
corpus <- tm_map(corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)})
cc[unlist(corpus)]
#建立文本資料結構與基本文字清洗
dir = DirSource("./", encoding = "BIG-5")
corpus = Corpus(dir)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)})
cc[unlist(corpus)]
tdm = TermDocumentMatrix(corpus)
tdm
inspect(tdm)
dir
dir[1]
corpus[[1]]
corpus[[1]]
View(corpus[[1]])
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)})
tdm = TermDocumentMatrix(corpus)
inspect(tdm)
help(TermDocumentMatrix)
library(tm)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
tdm = TermDocumentMatrix(corpus,
control = list(weighting = weightTfIdf,
stopwords = myStopwords,
removePunctuation = T,
removeNumbers = T,
stemming = T))
tdm = TermDocumentMatrix(corpus,
control = list(weighting = weightTfIdf,
#stopwords = myStopwords,
removePunctuation = T,
removeNumbers = T,
stemming = T))
inspect(tdm)
#建立文本資料結構與基本文字清洗
dir = DirSource("./", encoding = "UTF-8")
corpus = Corpus(dir)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)})
tdm = TermDocumentMatrix(corpus,
control = list(weighting = weightTfIdf,
#stopwords = myStopwords,
removePunctuation = T,
removeNumbers = T,
stemming = T))
inspect(tdm)
#建立文本資料結構與基本文字清洗
dir = DirSource("./", encoding = "UNICODE")
corpus = Corpus(dir)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
#建立文本資料結構與基本文字清洗
dir = DirSource("./", encoding = "BIG-5")
corpus = Corpus(dir)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)})
tdm = TermDocumentMatrix(corpus,
control = list(weighting = weightTfIdf,
#stopwords = myStopwords,
removePunctuation = T,
removeNumbers = T,
stemming = T))
inspect(tdm)
#進行斷詞，並依照日期建立文本矩陣 TermDocumentMatrix
mixseg = worker()
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
seg = lapply(corpus, jieba_tokenizer)
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
n = length(seg)
TDM = tokens[[1]]
colNames <- names(seg)
for( id in c(2:n) )
colNames <- gsub(".txt", "", colNames)
{
TDM = merge(TDM, tokens[[id]], by="d", all = TRUE)
names(TDM) = c('d', colNames[1:id])
}
TDM[is.na(TDM)] <- 0
kable(head(TDM))
library(knitr)
str(keywords)
# 斷詞
mixseg = worker()
jieba_tokenizer = function(d){
unlist(segment(d[[1]], mixseg))
}
seg = lapply(corpus, jieba_tokenizer)
#詞頻向量
freqFrame = as.data.frame(table(unlist(seg)))
str(freqFrame)
#詞頻矩陣
d.corpus <- Corpus(VectorSource(seg))
d.corpus
tdm <- TermDocumentMatrix(d.corpus)
inspect(tdm)
print( tf <- as.matrix(tdm) )
DF <- tidy(tf)
DF
#抓出排行榜上所有歌詞頁面連結所對應的網址
url  <- 'https://mojim.com/twzhot-song.htm'
library(ggplot2)
library(tidytext)
DF <- tidy(tf)
DF
#將已建好的 TDM 轉成 TF-IDF
tf <- apply(as.matrix(TDM[,2:(n+1)]), 2, sum)
library(Matrix)
#將已建好的 TDM 轉成 TF-IDF
tf <- apply(as.matrix(TDM[,2:(n+1)]), 2, sum)
library(stats)
library(proxy)
library(readtext)
library(slam)
#將已建好的 TDM 轉成 TF-IDF
tf <- apply(as.matrix(TDM[,2:(n+1)]), 2, sum)
N = tdm$ncol # Column數(資料數)
#將已建好的 TDM 轉成 TF-IDF
View(tdm)
N = tdm$ncol # Column數(資料數)
tf <- apply(tdm, 2, sum) # 2= 對每個Column
tf #每筆資料的資料量
idfCal <- function(word_doc)
{
log2( N / nnzero(word_doc) )    #'nnzero' = number of nonzero entries
}
idf <- apply(tdm, 1, idfCal) # 1= 對每個Row
doc.tfidf <- as.matrix(tdm)
doc.tfidf
for(x in 1:nrow(tdm))
{
for(y in 1:ncol(tdm))
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
}
findZeroId = as.matrix(apply(doc.tfidf, 1, sum)) # 1=對每個Row
findZeroId
tfidfnn = doc.tfidf[-which(findZeroId == 0),]
View(tfidfnn)
