findZeroId = as.matrix(apply(doc.tfidf, 1, sum)) # 1=對每個Row
findZeroId
tfidfnn = doc.tfidf[-which(findZeroId == 0),]
View(tfidfnn)
#TF-IDF Hours 文章取得的重要關鍵字
TopWords = data.frame()
for( id in c(1:n) ){
dayMax = order(tfidfnn[,id+1], decreasing = TRUE)
showResult = t(as.data.frame(tfidfnn[dayMax[1:5],1]))
TopWords = rbind(TopWords, showResult)
}
n = length(seg)
#TF-IDF Hours 文章取得的重要關鍵字
TopWords = data.frame()
for( id in c(1:n) ){
dayMax = order(tfidfnn[,id+1], decreasing = TRUE)
showResult = t(as.data.frame(tfidfnn[dayMax[1:5],1]))
TopWords = rbind(TopWords, showResult)
}
n
for( id in c(1:77) ){
dayMax = order(tfidfnn[,id+1], decreasing = TRUE)
showResult = t(as.data.frame(tfidfnn[dayMax[1:5],1]))
TopWords = rbind(TopWords, showResult)
}
for( id in c(1:77) ){
dayMax = order(tfidfnn[,id+1], decreasing = TRUE)
showResult = t(as.data.frame(tfidfnn[dayMax[1:5],1]))
TopWords = rbind(TopWords, showResult)
}
for( id in c(1:77) ){
dayMax = order(tfidfnn[,id+1], decreasing = TRUE)
showResult = t(as.data.frame(tfidfnn[dayMax[1:5],1]))
TopWords = rbind(TopWords, showResult)
}
View(tfidfnn)
dayMax = order(tfidfnn[,id], decreasing = TRUE)
for( id in c(1:77) ){
dayMax = order(tfidfnn[,id], decreasing = TRUE)
showResult = t(as.data.frame(tfidfnn[dayMax[1:5],1]))
TopWords = rbind(TopWords, showResult)
}
for( id in c(1:77) ){
dayMax = order(tfidfnn[,id], decreasing = TRUE)
showResult = t(as.data.frame(tfidfnn[dayMax[1:5],1]))
TopWords = rbind(TopWords, showResult)
}
###
freq=rowSums(as.matrix(tfidfnn))
head(freq,10)
tail(freq,10)
freq
#Plot those frequencies ordered.
plot(sort(freq, decreasing = T),col="blue",main="Word TF-IDF frequencies", xlab="TF-IDF-based rank", ylab = "TF-IDF")
#See the ten most frequent terms.
tail(sort(freq),n=10)
#Show most frequent terms and their frequencies in a bar plot.
high.freq=tail(sort(freq),n=10)
hfp.df=as.data.frame(sort(high.freq))
hfp.df
hfp.df$names <- rownames(hfp.df)
hfp.df
ggplot(hfp.df, aes(reorder(names,high.freq), high.freq)) +
geom_bar(stat="identity") + coord_flip() +
xlab("Terms") + ylab("Frequency") +
ggtitle("Term frequencies")
freq
as.data.frame(freq)
freqframe= as.data.frame(freq)
wordcloud(freqframe$Var1,freqframe$Freq,)
wordcloud(freqframe$Var1,freqframe$Freq)
library(wordcloud)
wordcloud(freqframe$Var1,freqframe$Freq)
wordcloud(freqframe$Var1,freqframe$Freq,
scale=c(5,0.5),min.freq=10,max.words=50,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE))
wordcloud(freqframe$Var1,freqframe$Freq,
scale=c(5,0.5),min.freq=10,max.words=50,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
str(freqframe)
freqframe= as.data.frame(freq)
freqframe
head(freqframe)
wordcloud(freqframe$1,freqframe$Freq,
scale=c(5,0.5),min.freq=10,max.words=50,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
str(freqframe)
library(c('bitops','httr'))
knitr::opts_chunk$set(echo = TRUE)
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
library(rvest)
library(magrittr)
library(ggplot2)
library(tidytext)
library(stats)
library(proxy)
library(readtext)
library(slam)
library(Matrix)
#抓出排行榜上所有歌詞頁面連結所對應的網址
url  <- 'https://mojim.com/twzhot-song.htm'
html <- htmlParse( GET(url) )
xpath= '//*[(@id = "mx5_A") and (((count(preceding-sibling::*) + 1) = 1) and parent::*)]//a[(((count(preceding-sibling::*) + 1) = 1) and parent::*)]'
url.list <- xpathSApply( html, xpath, xmlAttrs )
url.list
url.list[1,]
url  <- 'https://mojim.com/twzhot-song.htm'
html <- htmlParse( GET(url) )
xpath= '//*[(@id = "mx5_A") and (((count(preceding-sibling::*) + 1) = 1) and parent::*)]//a[(((count(preceding-sibling::*) + 1) = 1) and parent::*)]'
url.list <- xpathSApply( html, xpath, xmlAttrs )
url.list= url.list[1,]
url.list
url  <- 'https://mojim.com/twzhot-song.htm'
html <- htmlParse( GET(url) )
xpath= '//*[(@id = "mx5_A") and (((count(preceding-sibling::*) + 1) = 1) and parent::*)]//a[(((count(preceding-sibling::*) + 1) = 1) and parent::*)]'
url.list <- xpathSApply( html, xpath, xmlAttrs )
url.list= url.list[1,]
url.list
paster= function(a){paste0('https://mojim.com',a)}
url  <- 'https://mojim.com/twzhot-song.htm'
html <- htmlParse( GET(url) )
xpath= '//*[(@id = "mx5_A") and (((count(preceding-sibling::*) + 1) = 1) and parent::*)]//a[(((count(preceding-sibling::*) + 1) = 1) and parent::*)]'
url.list <- xpathSApply( html, xpath, xmlAttrs )
url.list= url.list[1,]
url.list
urls= list()
paster= function(a){paste0('https://mojim.com',a)}
urls= lapply(url.list, paster)
urls= unlist(urls)
urls
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
library(rvest)
library(magrittr)
library(ggplot2)
library(tidytext)
library(stats)
library(proxy)
library(readtext)
library(slam)
library(Matrix)
library(dplyr)
xpath2= '//*[(@id = "fsZx3")]'
xpath3= '//*[(@id = "fsZx2")]'
getdoc <- function(url){
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, xpath2, xmlValue )
title= xpathSApply(html, xpath3, xmlValue)
name <- paste0(title, ".txt")
write(doc, name)
}
lapply(urls,getdoc)
dir = DirSource("./", encoding = "BIG-5")
dir
corpus = Corpus(dir)
length(corpus)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)})
setwd("D:/CSX_Lyhs/week_5/hw_5/lyrics")
setwd("D:/CSX_Lyhs/week_5/hw_5/lyrics")
dir = DirSource("./", encoding = "BIG-5")
dir
corpus = Corpus(dir)
length(corpus)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)})
setwd("D:/CSX_Lyhs/week_5/hw_5/lyrics")
dir = DirSource("./", encoding = "BIG-5")
dir
corpus = Corpus(dir)
length(corpus)   #爬蟲失敗23個
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)})
setwd("D:/CSX_Lyhs/week_5/hw_5/lyrics")
dir = DirSource("./", encoding = "BIG-5")
dir
corpus = Corpus(dir)
length(corpus)   #爬蟲失敗23個
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)})
setwd("D:/CSX_Lyhs/week_5/hw_5/lyrics")
knitr::opts_chunk$set(echo = TRUE)
setwd("D:/CSX_Lyhs/week_5/hw_5/lyrics")
dir = DirSource("./", encoding = "BIG-5")
dir
corpus = Corpus(dir)
length(corpus)   #爬蟲失敗23個
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)})
knitr::opts_chunk$set(echo = TRUE)
setwd("D:/CSX_Lyhs/week_5/hw_5/lyrics")
dir = DirSource("./", encoding = "BIG-5")
dir
corpus = Corpus(dir)
length(corpus)   #爬蟲失敗23個
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)})
corpus[[1]]
setwd("D:/CSX_Lyhs/week_5/hw_5/lyrics")
dir = DirSource("./", encoding = "BIG-5")
dir
corpus = Corpus(dir)
length(corpus)   #爬蟲失敗23個
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)})
corpus[[1]]
mixseg = worker()
jieba_tokenizer = function(d){
unlist(segment(d[[1]], mixseg))
}
seg = lapply(corpus, jieba_tokenizer)
n = length(seg)
mixseg = worker()
jieba_tokenizer = function(d){
unlist(segment(d[[1]], mixseg))
}
seg = lapply(corpus, jieba_tokenizer)
n = length(seg)
n
freqFrame = as.data.frame(table(unlist(seg)))
str(freqFrame)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame
freqFrame = as.data.frame(table(unlist(seg)))
head(freqFrame)
tail(freqFrame)
freqFrame = as.data.frame(table(unlist(seg)))
tail(freqFrame)
d.corpus <- Corpus(VectorSource(seg))
d.corpus
tdm <- TermDocumentMatrix(d.corpus)
inspect(tdm)
print( tf <- as.matrix(tdm) )
DF <- tidy(tf)
DF
d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus)
print( tf <- as.matrix(tdm) )
DF <- tidy(tf)
DF
d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus)
tf <- as.matrix(tdm)
DF <- tidy(tf)
DF
N = tdm$ncol
tf <- apply(tdm, 2, sum)
tf
idfCal <- function(word_doc){
log2( N / nnzero(word_doc) )
}
idf <- apply(tdm, 1, idfCal)
doc.tfidf <- as.matrix(tdm)
doc.tfidf
for(x in 1:nrow(tdm)){
for(y in 1:ncol(tdm))
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
}
findZeroId = as.matrix(apply(doc.tfidf, 1, sum)) # 1=對每個Row
findZeroId
tfidfnn = doc.tfidf[-which(findZeroId == 0),]
View(tfidfnn)
N = tdm$ncol
tf <- apply(tdm, 2, sum)
idfCal <- function(word_doc){
log2( N / nnzero(word_doc) )
}
idf <- apply(tdm, 1, idfCal)
doc.tfidf <- as.matrix(tdm)
for(x in 1:nrow(tdm)){
for(y in 1:ncol(tdm))
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
}
findZeroId = as.matrix(apply(doc.tfidf, 1, sum)) # 1=對每個Row
tfidfnn = doc.tfidf[-which(findZeroId == 0),]
tfidfnn
N = tdm$ncol
tf <- apply(tdm, 2, sum)
idfCal <- function(word_doc){
log2( N / nnzero(word_doc) )
}
idf <- apply(tdm, 1, idfCal)
doc.tfidf <- as.matrix(tdm)
for(x in 1:nrow(tdm)){
for(y in 1:ncol(tdm))
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
}
findZeroId = as.matrix(apply(doc.tfidf, 1, sum)) # 1=對每個Row
tfidfnn = doc.tfidf[-which(findZeroId == 0),]
tfidfnn[,1:8]
N = tdm$ncol
tf <- apply(tdm, 2, sum)
idfCal <- function(word_doc){
log2( N / nnzero(word_doc) )
}
idf <- apply(tdm, 1, idfCal)
doc.tfidf <- as.matrix(tdm)
for(x in 1:nrow(tdm)){
for(y in 1:ncol(tdm))
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
}
findZeroId = as.matrix(apply(doc.tfidf, 1, sum)) # 1=對每個Row
tfidfnn = doc.tfidf[-which(findZeroId == 0),]
tfidfnn[1:8,1:8]
N = tdm$ncol
tf <- apply(tdm, 2, sum)
idfCal <- function(word_doc){
log2( N / nnzero(word_doc) )
}
idf <- apply(tdm, 1, idfCal)
doc.tfidf <- as.matrix(tdm)
for(x in 1:nrow(tdm)){
for(y in 1:ncol(tdm))
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
}
findZeroId = as.matrix(apply(doc.tfidf, 1, sum)) # 1=對每個Row
tfidfnn = doc.tfidf[-which(findZeroId == 0),]
tfidfnn[1:10,1:10]
N = tdm$ncol
tf <- apply(tdm, 2, sum)
idfCal <- function(word_doc){
log2( N / nnzero(word_doc) )
}
idf <- apply(tdm, 1, idfCal)
doc.tfidf <- as.matrix(tdm)
for(x in 1:nrow(tdm)){
for(y in 1:ncol(tdm))
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
}
findZeroId = as.matrix(apply(doc.tfidf, 1, sum)) # 1=對每個Row
tfidfnn = doc.tfidf[-which(findZeroId == 0),]
tfidfnn[1:15,1:15]
N = tdm$ncol
tf <- apply(tdm, 2, sum)
idfCal <- function(word_doc){
log2( N / nnzero(word_doc) )
}
idf <- apply(tdm, 1, idfCal)
doc.tfidf <- as.matrix(tdm)
for(x in 1:nrow(tdm)){
for(y in 1:ncol(tdm))
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
}
findZeroId = as.matrix(apply(doc.tfidf, 1, sum)) # 1=對每個Row
tfidfnn = doc.tfidf[-which(findZeroId == 0),]
tfidfnn[1:11,1:11]
#
freq=rowSums(as.matrix(tfidfnn))
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
library(rvest)
library(magrittr)
library(ggplot2)
library(tidytext)
library(stats)
library(proxy)
library(readtext)
library(slam)
library(Matrix)
library(dplyr)
xpath2= '//*[(@id = "fsZx3")]'
xpath3= '//*[(@id = "fsZx2")]'
getdoc <- function(url){
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, xpath2, xmlValue )
title= xpathSApply(html, xpath3, xmlValue)
name <- paste0(title, ".txt")
#write(doc, name)
}
lapply(urls,getdoc)
xpath2= '//*[(@id = "fsZx3")]'
xpath3= '//*[(@id = "fsZx2")]'
getdoc <- function(url){
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, xpath2, xmlValue )
title= xpathSApply(html, xpath3, xmlValue)
name <- paste0(title, ".txt")
write(doc, name)
}
#lapply(urls,getdoc)
xpath2= '//*[(@id = "fsZx3")]'
xpath3= '//*[(@id = "fsZx2")]'
getdoc <- function(url){
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, xpath2, xmlValue )
title= xpathSApply(html, xpath3, xmlValue)
name <- paste0(title, ".txt")
write(doc, name)
}
lapply(urls,getdoc)
xpath2= '//*[(@id = "fsZx3")]'
xpath3= '//*[(@id = "fsZx2")]'
getdoc <- function(url){
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, xpath2, xmlValue )
title= xpathSApply(html, xpath3, xmlValue)
name <- paste0(title, ".txt")
write(doc, name)
}
#lapply(urls,getdoc)
setwd("D:/CSX_Lyhs/week_5/hw_5/lyrics")
dir = DirSource("./", encoding = "BIG-5")
dir
corpus = Corpus(dir)
length(corpus)   #爬蟲失敗23個
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)})
freq=rowSums(as.matrix(tfidfnn))
head(freq,10)
#Plot those frequencies ordered.
plot(sort(freq, decreasing = T),col="blue",main="Word TF-IDF frequencies", xlab="TF-IDF-based rank", ylab = "TF-IDF")
high.freq=tail(sort(freq),n=10)
hfp.df=as.data.frame(sort(high.freq))
hfp.df$names <- rownames(hfp.df)
ggplot(hfp.df, aes(reorder(names,high.freq), high.freq)) +
geom_bar(stat="identity") + coord_flip() +
xlab("Terms") + ylab("Frequency") +
ggtitle("Term frequencies")
freq=rowSums(as.matrix(tfidfnn))
head(freq,10)
high.freq=tail(sort(freq),n=10)
hfp.df=as.data.frame(sort(high.freq))
hfp.df$names <- rownames(hfp.df)
ggplot(hfp.df, aes(reorder(names,high.freq), high.freq)) +
geom_bar(stat="identity") + coord_flip() +
xlab("Terms") + ylab("Frequency") +
ggtitle("關鍵詞")
plot(sort(freq, decreasing = T),col="blue",main="關鍵程度", xlab="TF-IDF-based rank", ylab = "TF-IDF")
mixseg = worker()
jieba_tokenizer = function(d){
unlist(segment(d[[1]], mixseg))
}
seg = lapply(corpus, jieba_tokenizer)
seg
mixseg = worker()
jieba_tokenizer = function(d){
unlist(segment(d[[1]], mixseg))
}
seg = lapply(corpus, jieba_tokenizer)
head(seg)
setwd("D:/CSX_Lyhs/week_5/hw_5/lyrics")
dir = DirSource("./", encoding = "BIG-5")
dir
corpus = Corpus(dir)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)})
corpus
